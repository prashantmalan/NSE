{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMNzCAQakWrKaKjicPkk9y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prashantmalan/NSE/blob/main/Anomalywithdash1610224_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "yOcZcSN9GTK0",
        "outputId": "0ec46abc-825d-420c-d1cd-ba7ab0481c9b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'jupyter_dash'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2cbf64f3d8ad>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjupyter_dash\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJupyterDash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdash\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdash\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdependencies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jupyter_dash'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from jupyter_dash import JupyterDash\n",
        "from dash import dcc, html\n",
        "from dash.dependencies import Input, Output, State\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import os\n",
        "\n",
        "# Load and preprocess data\n",
        "def read_data(transaction_url, classification_url):\n",
        "    transaction_data = pd.read_csv(transaction_url)\n",
        "    column_classification = pd.read_csv(classification_url)\n",
        "    return transaction_data, column_classification\n",
        "\n",
        "def preprocess_data(transaction_data, column_classification):\n",
        "    transaction_data = transaction_data[~transaction_data.applymap(lambda x: ';' in str(x)).any(axis=1)]\n",
        "    transaction_data.reset_index(drop=True, inplace=True)\n",
        "    transaction_data = transaction_data.applymap(lambda x: str(x).replace(',', '') if isinstance(x, str) else x)\n",
        "    transaction_data = transaction_data.applymap(lambda x: str(x).replace('+', '') if isinstance(x, str) else x)\n",
        "    transaction_data = transaction_data[~transaction_data.applymap(lambda x: ';' in str(x)).any(axis=1)]\n",
        "\n",
        "    feature_categories = column_classification.set_index('Column')['Type'].to_dict()\n",
        "    numeric_features = [col for col, cat in feature_categories.items() if cat == 'Amt']\n",
        "    numeric_features = [col for col in numeric_features if col in transaction_data.columns]\n",
        "\n",
        "    all_nan_columns = transaction_data[numeric_features].columns[transaction_data[numeric_features].isna().all()].tolist()\n",
        "    transaction_data[all_nan_columns] = 0\n",
        "\n",
        "    return transaction_data, numeric_features\n",
        "\n",
        "def create_test_data(transaction_data, numeric_features, multiplier=-1):\n",
        "    test_data = transaction_data.tail(100).copy()\n",
        "    for feature in numeric_features:\n",
        "        test_data[feature] = pd.to_numeric(test_data[feature], errors='coerce')\n",
        "    test_data[numeric_features] = test_data[numeric_features].fillna(0)\n",
        "    test_data['Total notional quantity-Leg 1'] = test_data['Total notional quantity-Leg 1'].apply(lambda x: x * multiplier)\n",
        "    return test_data\n",
        "\n",
        "# Initialize data\n",
        "transaction_data, column_classification = read_data(\n",
        "    'https://raw.githubusercontent.com/prashantmalan/NSE/main/Pm_CFTC_2017649.csv',\n",
        "    'https://raw.githubusercontent.com/prashantmalan/NSE/main/col_cat_5.csv'\n",
        ")\n",
        "transaction_data, numeric_features = preprocess_data(transaction_data, column_classification)\n",
        "test_data = create_test_data(transaction_data, numeric_features)\n",
        "\n",
        "# Initialize the Dash app\n",
        "app = JupyterDash(__name__)\n",
        "\n",
        "# Layout\n",
        "app.layout = html.Div([\n",
        "    html.H1(\"ML Pipeline Dashboard\", style={'text-align': 'center', 'color': '#4CAF50'}),\n",
        "    html.Div([\n",
        "        html.Div([\n",
        "            html.H3(\"Configure and Run Model\", style={'color': '#2196F3'}),\n",
        "            dcc.Dropdown(\n",
        "                id='model-dropdown',\n",
        "                options=[\n",
        "                    {'label': 'Isolation Forest', 'value': 'isolation_forest'}\n",
        "                ],\n",
        "                value='isolation_forest',\n",
        "                style={'width': '100%', 'margin-bottom': '10px'}\n",
        "            ),\n",
        "            dcc.Input(\n",
        "                id='model-param-input',\n",
        "                type='text',\n",
        "                placeholder='Enter contamination level (e.g. 0.05)',\n",
        "                value='0.05',\n",
        "                style={'width': '100%', 'margin-bottom': '10px'}\n",
        "            ),\n",
        "            html.Div([\n",
        "                html.Button('Train Model', id='train-button', n_clicks=0, style={'width': '48%', 'margin-right': '4%', 'background-color': '#1f77b4', 'color': 'white', 'border': 'none', 'padding': '10px', 'border-radius': '5px'}),\n",
        "                html.Button('Run Prediction', id='predict-button', n_clicks=0, style={'width': '48%', 'background-color': '#ff7f0e', 'color': 'white', 'border': 'none', 'padding': '10px', 'border-radius': '5px'})\n",
        "            ], style={'display': 'flex', 'justify-content': 'space-between', 'margin-bottom': '10px'}),\n",
        "            dcc.Loading(\n",
        "                id=\"loading-1\",\n",
        "                type=\"circle\",\n",
        "                children=[\n",
        "                    html.Div(id='training-status', style={'text-align': 'center', 'margin-bottom': '10px', 'color': '#FF5722'})\n",
        "                ]\n",
        "            ),\n",
        "            dcc.Graph(id='train-graph'),\n",
        "            dcc.Loading(\n",
        "                id=\"loading-2\",\n",
        "                type=\"circle\",\n",
        "                children=[\n",
        "                    html.Div(id='prediction-status', style={'text-align': 'center', 'margin-bottom': '10px', 'color': '#FF5722'})\n",
        "                ]\n",
        "            ),\n",
        "            dcc.Graph(id='prediction-graph')\n",
        "        ], style={'width': '100%', 'padding': '20px', 'border': '1px solid #ccc', 'border-radius': '5px', 'box-shadow': '2px 2px 5px lightgrey'})\n",
        "    ], style={'width': '80%', 'margin': 'auto'})\n",
        "], style={'width': '100%', 'height': '100vh', 'background-color': '#f0f0f0'})\n",
        "\n",
        "def plot_anomalies(transaction_data, anomaly_scores_df, numeric_features):\n",
        "    n_features = len(numeric_features)\n",
        "    n_cols = 4\n",
        "    n_rows = int(np.ceil(n_features / n_cols))\n",
        "    fig = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=numeric_features, horizontal_spacing=0.05, vertical_spacing=0.05)\n",
        "\n",
        "    for i, feature in enumerate(numeric_features):\n",
        "        row = i // n_cols + 1\n",
        "        col = i % n_cols + 1\n",
        "\n",
        "        sorted_indices = transaction_data[feature].sort_values().index\n",
        "        normal_data = pd.to_numeric(transaction_data.loc[sorted_indices, feature][anomaly_scores_df[feature] == 1])\n",
        "        anomalous_data = pd.to_numeric(transaction_data.loc[sorted_indices, feature][anomaly_scores_df[feature] == -1])\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=sorted_indices[anomaly_scores_df[feature] == 1],\n",
        "            y=normal_data,\n",
        "            mode='markers',\n",
        "            name='Normal',\n",
        "            customdata=anomaly_scores_df.loc[sorted_indices][anomaly_scores_df[feature] == 1][['Product name', 'Action type']],\n",
        "            hovertemplate='Product name: %{customdata[0]}<br>Action type: %{customdata[1]}<br>Value: %{y}<extra></extra>',\n",
        "            marker=dict(size=8, color='green', opacity=0.7)\n",
        "        ), row=row, col=col)\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=sorted_indices[anomaly_scores_df[feature] == -1],\n",
        "            y=anomalous_data,\n",
        "            mode='markers',\n",
        "            name='Anomalous',\n",
        "            customdata=anomaly_scores_df.loc[sorted_indices][anomaly_scores_df[feature] == -1][['Product name', 'Action type']],\n",
        "            hovertemplate='Product name: %{customdata[0]}<br>Action type: %{customdata[1]}<br>Value: %{y}<extra></extra>',\n",
        "            marker=dict(size=8, color='red', opacity=0.7)\n",
        "        ), row=row, col=col)\n",
        "\n",
        "        fig.update_yaxes(range=[normal_data.min(), normal_data.max()], row=row, col=col)\n",
        "\n",
        "    fig.update_layout(\n",
        "        title='Anomaly Scores for Each Feature',\n",
        "        showlegend=False,\n",
        "        hovermode='closest',\n",
        "        font=dict(size=10),\n",
        "        autosize=False,\n",
        "        width=1500,\n",
        "        height=1200\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "def load_and_predict(transaction_data, numeric_features, model_dir='models'):\n",
        "    anomaly_scores_df = pd.DataFrame(index=transaction_data.index)\n",
        "    for feature in numeric_features:\n",
        "        model_path = os.path.join(model_dir, f'model_anomaly_{feature}.joblib')\n",
        "        pipeline = joblib.load(model_path)\n",
        "        feature_data = transaction_data[[feature]].copy()\n",
        "        scores = pipeline.predict(feature_data)\n",
        "        anomaly_scores_df[feature] = scores\n",
        "    anomaly_scores_df['Product name'] = transaction_data['Product name']\n",
        "    anomaly_scores_df['Action type'] = transaction_data['Action type']\n",
        "    return anomaly_scores_df\n",
        "\n",
        "# Train models and plot\n",
        "@app.callback(\n",
        "    [Output('train-graph', 'figure'),\n",
        "     Output('training-status', 'children')],\n",
        "    [Input('train-button', 'n_clicks')],\n",
        "    [State('model-param-input', 'value')]\n",
        ")\n",
        "def train_models(n_clicks, contamination_level):\n",
        "    if n_clicks > 0:\n",
        "        model_dir = 'models'\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        pipeline_template = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='mean')),\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('isolation_forest', IsolationForest(contamination=float(contamination_level), random_state=42))\n",
        "        ])\n",
        "\n",
        "        for feature in numeric_features:\n",
        "            feature_data = transaction_data[[feature]].copy()\n",
        "            pipeline = pipeline_template.fit(feature_data)\n",
        "            model_path = os.path.join(model_dir, f'model_anomaly_{feature}.joblib')\n",
        "            joblib.dump(pipeline, model_path)\n",
        "\n",
        "        # Predict anomalies on training data for visualization\n",
        "        anomaly_scores_df = load_and_predict(transaction_data, numeric_features)\n",
        "        return plot_anomalies(transaction_data, anomaly_scores_df, numeric_features), \"Training Complete\"\n",
        "\n",
        "    return go.Figure(), \"Awaiting Training\"\n",
        "\n",
        "# Test models and plot\n",
        "@app.callback(\n",
        "    [Output('prediction-graph', 'figure'),\n",
        "     Output('prediction-status', 'children')],\n",
        "    [Input('predict-button', 'n_clicks')]\n",
        ")\n",
        "def test_models(n_clicks):\n",
        "    if n_clicks > 0:\n",
        "        # Predict anomalies on test data\n",
        "        anomaly_scores_df = load_and_predict(test_data, numeric_features)\n",
        "        return plot_anomalies(test_data, anomaly_scores_df, numeric_features), \"Prediction Complete\"\n",
        "\n",
        "    return go.Figure(), \"Awaiting Prediction\"\n",
        "\n",
        "# Run the app\n",
        "app.run_server(mode='inline')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jupyter_dash import JupyterDash\n",
        "from dash import dcc, html\n",
        "from dash.dependencies import Input, Output, State\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import os\n",
        "\n",
        "# Load and preprocess data\n",
        "def read_data(transaction_url, classification_url):\n",
        "    transaction_data = pd.read_csv(transaction_url)\n",
        "    column_classification = pd.read_csv(classification_url)\n",
        "    return transaction_data, column_classification\n",
        "\n",
        "def preprocess_data(transaction_data, column_classification):\n",
        "    transaction_data = transaction_data[~transaction_data.applymap(lambda x: ';' in str(x)).any(axis=1)]\n",
        "    transaction_data.reset_index(drop=True, inplace=True)\n",
        "    transaction_data = transaction_data.applymap(lambda x: str(x).replace(',', '') if isinstance(x, str) else x)\n",
        "    transaction_data = transaction_data.applymap(lambda x: str(x).replace('+', '') if isinstance(x, str) else x)\n",
        "    transaction_data = transaction_data[~transaction_data.applymap(lambda x: ';' in str(x)).any(axis=1)]\n",
        "\n",
        "    feature_categories = column_classification.set_index('Column')['Type'].to_dict()\n",
        "    numeric_features = [col for col, cat in feature_categories.items() if cat == 'Amt']\n",
        "    numeric_features = [col for col in numeric_features if col in transaction_data.columns]\n",
        "\n",
        "    all_nan_columns = transaction_data[numeric_features].columns[transaction_data[numeric_features].isna().all()].tolist()\n",
        "    transaction_data[all_nan_columns] = 0\n",
        "\n",
        "    return transaction_data, numeric_features\n",
        "\n",
        "def create_test_data(transaction_data, numeric_features, multiplier=-1):\n",
        "    test_data = transaction_data.tail(100).copy()\n",
        "    for feature in numeric_features:\n",
        "        test_data[feature] = pd.to_numeric(test_data[feature], errors='coerce')\n",
        "    test_data[numeric_features] = test_data[numeric_features].fillna(0)\n",
        "    test_data['Total notional quantity-Leg 1'] = test_data['Total notional quantity-Leg 1'].apply(lambda x: x * multiplier)\n",
        "    return test_data\n",
        "\n",
        "# Initialize data\n",
        "transaction_data, column_classification = read_data(\n",
        "    'https://raw.githubusercontent.com/prashantmalan/NSE/main/Pm_CFTC_2017649.csv',\n",
        "    'https://raw.githubusercontent.com/prashantmalan/NSE/main/col_cat_5.csv'\n",
        ")\n",
        "transaction_data, numeric_features = preprocess_data(transaction_data, column_classification)\n",
        "test_data = create_test_data(transaction_data, numeric_features)\n",
        "\n",
        "# Initialize the Dash app\n",
        "app = JupyterDash(__name__)\n",
        "\n",
        "# Layout\n",
        "app.layout = html.Div([\n",
        "    html.H1(\"ML Pipeline Dashboard\", style={'text-align': 'center', 'color': '#4CAF50'}),\n",
        "    html.Div([\n",
        "        html.Div([\n",
        "            html.H3(\"Configure and Run Model\", style={'color': '#2196F3'}),\n",
        "            dcc.Dropdown(\n",
        "                id='model-dropdown',\n",
        "                options=[\n",
        "                    {'label': 'Isolation Forest', 'value': 'isolation_forest'}\n",
        "                ],\n",
        "                value='isolation_forest',\n",
        "                style={'width': '100%', 'margin-bottom': '10px'}\n",
        "            ),\n",
        "            dcc.Input(\n",
        "                id='model-param-input',\n",
        "                type='text',\n",
        "                placeholder='Enter contamination level (e.g. 0.05)',\n",
        "                value='0.05',\n",
        "                style={'width': '100%', 'margin-bottom': '10px'}\n",
        "            ),\n",
        "            html.Div([\n",
        "                html.Button('Train Model', id='train-button', n_clicks=0, style={'width': '48%', 'margin-right': '4%', 'background-color': '#1f77b4', 'color': 'white', 'border': 'none', 'padding': '10px', 'border-radius': '5px'}),\n",
        "                html.Button('Run Prediction', id='predict-button', n_clicks=0, style={'width': '48%', 'background-color': '#ff7f0e', 'color': 'white', 'border': 'none', 'padding': '10px', 'border-radius': '5px'})\n",
        "            ], style={'display': 'flex', 'justify-content': 'space-between', 'margin-bottom': '10px'}),\n",
        "            dcc.Loading(\n",
        "                id=\"loading-1\",\n",
        "                type=\"circle\",\n",
        "                children=[\n",
        "                    html.Div(id='training-status', style={'text-align': 'center', 'margin-bottom': '10px', 'color': '#FF5722'})\n",
        "                ]\n",
        "            ),\n",
        "            dcc.Graph(id='train-graph'),\n",
        "            dcc.Loading(\n",
        "                id=\"loading-2\",\n",
        "                type=\"circle\",\n",
        "                children=[\n",
        "                    html.Div(id='prediction-status', style={'text-align': 'center', 'margin-bottom': '10px', 'color': '#FF5722'})\n",
        "                ]\n",
        "            ),\n",
        "            dcc.Graph(id='prediction-graph')\n",
        "        ], style={'width': '100%', 'padding': '20px', 'border': '1px solid #ccc', 'border-radius': '5px', 'box-shadow': '2px 2px 5px lightgrey'})\n",
        "    ], style={'width': '80%', 'margin': 'auto'})\n",
        "], style={'width': '100%', 'height': '100vh', 'background-color': '#f0f0f0'})\n",
        "\n",
        "def plot_anomalies(transaction_data, anomaly_scores_df, numeric_features):\n",
        "    n_features = len(numeric_features)\n",
        "    n_cols = 4\n",
        "    n_rows = int(np.ceil(n_features / n_cols))\n",
        "    fig = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=numeric_features, horizontal_spacing=0.05, vertical_spacing=0.05)\n",
        "\n",
        "    for i, feature in enumerate(numeric_features):\n",
        "        row = i // n_cols + 1\n",
        "        col = i % n_cols + 1\n",
        "\n",
        "        sorted_indices = transaction_data[feature].sort_values().index\n",
        "        normal_data = pd.to_numeric(transaction_data.loc[sorted_indices, feature][anomaly_scores_df[feature] == 1])\n",
        "        anomalous_data = pd.to_numeric(transaction_data.loc[sorted_indices, feature][anomaly_scores_df[feature] == -1])\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=sorted_indices[anomaly_scores_df[feature] == 1],\n",
        "            y=normal_data,\n",
        "            mode='markers',\n",
        "            name='Normal',\n",
        "            customdata=anomaly_scores_df.loc[sorted_indices][anomaly_scores_df[feature] == 1][['Product name', 'Action type']],\n",
        "            hovertemplate='Product name: %{customdata[0]}<br>Action type: %{customdata[1]}<br>Value: %{y}<extra></extra>',\n",
        "            marker=dict(size=8, color='green', opacity=0.7)\n",
        "        ), row=row, col=col)\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=sorted_indices[anomaly_scores_df[feature] == -1],\n",
        "            y=anomalous_data,\n",
        "            mode='markers',\n",
        "            name='Anomalous',\n",
        "            customdata=anomaly_scores_df.loc[sorted_indices][anomaly_scores_df[feature] == -1][['Product name', 'Action type']],\n",
        "            hovertemplate='Product name: %{customdata[0]}<br>Action type: %{customdata[1]}<br>Value: %{y}<extra></extra>',\n",
        "            marker=dict(size=8, color='red', opacity=0.7)\n",
        "        ), row=row, col=col)\n",
        "\n",
        "        fig.update_yaxes(range=[normal_data.min(), normal_data.max()], row=row, col=col)\n",
        "\n",
        "    fig.update_layout(\n",
        "        title='Anomaly Scores for Each Feature',\n",
        "        showlegend=False,\n",
        "        hovermode='closest',\n",
        "        font=dict(size=10),\n",
        "        autosize=False,\n",
        "        width=1500,\n",
        "        height=1200\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "def load_and_predict(transaction_data, numeric_features, model_dir='models'):\n",
        "    anomaly_scores_df = pd.DataFrame(index=transaction_data.index)\n",
        "    for feature in numeric_features:\n",
        "        model_path = os.path.join(model_dir, f'model_anomaly_{feature}.joblib')\n",
        "        pipeline = joblib.load(model_path)\n",
        "        feature_data = transaction_data[[feature]].copy()\n",
        "        scores = pipeline.predict(feature_data)\n",
        "        anomaly_scores_df[feature] = scores\n",
        "    anomaly_scores_df['Product name'] = transaction_data['Product name']\n",
        "    anomaly_scores_df['Action type'] = transaction_data['Action type']\n",
        "    return anomaly_scores_df\n",
        "\n",
        "# Train models and plot\n",
        "@app.callback(\n",
        "    [Output('train-graph', 'figure'),\n",
        "     Output('training-status', 'children')],\n",
        "    [Input('train-button', 'n_clicks')],\n",
        "    [State('model-param-input', 'value')]\n",
        ")\n",
        "def train_models(n_clicks, contamination_level):\n",
        "    if n_clicks > 0:\n",
        "        model_dir = 'models'\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        pipeline_template = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='mean')),\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('isolation_forest', IsolationForest(contamination=float(contamination_level), random_state=42))\n",
        "        ])\n",
        "\n",
        "        for feature in numeric_features:\n",
        "            feature_data = transaction_data[[feature]].copy()\n",
        "            pipeline = pipeline_template.fit(feature_data)\n",
        "            model_path = os.path.join(model_dir, f'model_anomaly_{feature}.joblib')\n",
        "            joblib.dump(pipeline, model_path)\n",
        "\n",
        "        # Predict anomalies on training data for visualization\n",
        "        anomaly_scores_df = load_and_predict(transaction_data, numeric_features)\n",
        "        return plot_anomalies(transaction_data, anomaly_scores_df, numeric_features), \"Training Complete\"\n",
        "\n",
        "    return go.Figure(), \"Awaiting Training\"\n",
        "\n",
        "# Test models and plot\n",
        "@app.callback(\n",
        "    [Output('prediction-graph', 'figure'),\n",
        "     Output('prediction-status', 'children')],\n",
        "    [Input('predict-button', 'n_clicks')]\n",
        ")\n",
        "def test_models(n_clicks):\n",
        "    if n_clicks > 0:\n",
        "        # Predict anomalies on test data\n",
        "        anomaly_scores_df = load_and_predict(test_data, numeric_features)\n",
        "        return plot_anomalies(test_data, anomaly_scores_df, numeric_features), \"Prediction Complete\"\n",
        "\n",
        "    return go.Figure(), \"Awaiting Prediction\"\n",
        "\n",
        "# Run the app\n",
        "app.run_server(mode='inline', debug=True)"
      ],
      "metadata": {
        "id": "_hpSbI2-Gta3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "90c3a9da-3f90-4eca-e856-a53c05c49949"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-a3a8acb5d793>:17: DtypeWarning: Columns (9,17,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  transaction_data = pd.read_csv(transaction_url)\n",
            "<ipython-input-3-a3a8acb5d793>:22: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  transaction_data = transaction_data[~transaction_data.applymap(lambda x: ';' in str(x)).any(axis=1)]\n",
            "<ipython-input-3-a3a8acb5d793>:24: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  transaction_data = transaction_data.applymap(lambda x: str(x).replace(',', '') if isinstance(x, str) else x)\n",
            "<ipython-input-3-a3a8acb5d793>:25: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  transaction_data = transaction_data.applymap(lambda x: str(x).replace('+', '') if isinstance(x, str) else x)\n",
            "<ipython-input-3-a3a8acb5d793>:26: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  transaction_data = transaction_data[~transaction_data.applymap(lambda x: ';' in str(x)).any(axis=1)]\n",
            "/usr/local/lib/python3.10/dist-packages/dash/dash.py:585: UserWarning:\n",
            "\n",
            "JupyterDash is deprecated, use Dash instead.\n",
            "See https://dash.plotly.com/dash-in-jupyter for more details.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8050, \"/\", \"100%\", 650, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8050, \"/\", \"100%\", 650, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jupyter-dash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA6_S5D3BdY4",
        "outputId": "a4907387-14b9-434b-eece-d181bdc66dd5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jupyter-dash\n",
            "  Downloading jupyter_dash-0.4.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dash (from jupyter-dash)\n",
            "  Downloading dash-2.18.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from jupyter-dash) (2.32.3)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from jupyter-dash) (2.2.5)\n",
            "Collecting retrying (from jupyter-dash)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from jupyter-dash) (7.34.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter-dash) (5.5.6)\n",
            "Collecting ansi2html (from jupyter-dash)\n",
            "  Downloading ansi2html-1.9.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from jupyter-dash) (1.6.0)\n",
            "Requirement already satisfied: Werkzeug<3.1 in /usr/local/lib/python3.10/dist-packages (from dash->jupyter-dash) (3.0.4)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash->jupyter-dash) (5.24.1)\n",
            "Collecting dash-html-components==2.0.0 (from dash->jupyter-dash)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash->jupyter-dash)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting dash-table==5.0.0 (from dash->jupyter-dash)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash->jupyter-dash) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash->jupyter-dash) (4.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from dash->jupyter-dash) (71.0.4)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask->jupyter-dash) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->jupyter-dash) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->jupyter-dash) (8.1.7)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter-dash) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter-dash) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter-dash) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter-dash) (6.3.3)\n",
            "Collecting jedi>=0.16 (from ipython->jupyter-dash)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->jupyter-dash) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->jupyter-dash) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->jupyter-dash) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->jupyter-dash) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->jupyter-dash) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->jupyter-dash) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->jupyter-dash) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->jupyter-dash) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->jupyter-dash) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->jupyter-dash) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->jupyter-dash) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying->jupyter-dash) (1.16.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->jupyter-dash) (0.8.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask->jupyter-dash) (3.0.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->jupyter-dash) (0.7.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash->jupyter-dash) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash->jupyter-dash) (24.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->jupyter-dash) (0.2.13)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash->jupyter-dash) (3.20.2)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->jupyter-dash) (5.7.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->jupyter-dash) (24.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->jupyter-dash) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.0->jupyter-client->ipykernel->jupyter-dash) (4.3.6)\n",
            "Downloading jupyter_dash-0.4.2-py3-none-any.whl (23 kB)\n",
            "Downloading ansi2html-1.9.2-py3-none-any.whl (17 kB)\n",
            "Downloading dash-2.18.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dash-table, dash-html-components, dash-core-components, retrying, jedi, ansi2html, dash, jupyter-dash\n",
            "Successfully installed ansi2html-1.9.2 dash-2.18.1 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 jedi-0.19.1 jupyter-dash-0.4.2 retrying-1.3.4\n"
          ]
        }
      ]
    }
  ]
}